{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# resdir = r'C:\\Users\\jiahuic\\Dropbox (University of Michigan)\\Ford_CC\\\\'\n",
    "resdir = r'F:\\University of Michigan Dropbox\\Jiahui Chen\\Ford_CC\\\\'\n",
    "# resdir = r'G:\\Dropbox (University of Michigan)\\Ford_CC\\\\'\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    counties = json.load(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaspricedf = pd.read_csv(resdir+r'\\\\CostEmissionAssessment_revision\\U.S._Regular_All_Formulations_Retail_Gasoline_Prices.csv',skiprows=5)\n",
    "gasprice = np.mean(gaspricedf['Prices'][0:36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qual lifetime\n",
    "# EV aggregate\n",
    "resdir = r'F:\\University of Michigan Dropbox\\Jiahui Chen\\Ford_CC\\\\'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "# Read base data\n",
    "ctdf_withcarb = pd.read_csv(resdir+r'\\\\2020_Gaz_counties_national_0728_tempreg.csv')\n",
    "uqlst = np.unique(ctdf_withcarb['tempreg'])\n",
    "\n",
    "# Pre-allocate columns to avoid fragmentation\n",
    "columns_to_add = []\n",
    "for year in ['24','30','40']:\n",
    "    for scen in ['UC','CC','V2H','V2Hnoelec','V2H_cons']:\n",
    "        columns_to_add.extend([\n",
    "            f'{year}{scen}costs', f'{year}{scen}emissions'\n",
    "        ])\n",
    "\n",
    "# Initialize all columns with zeros\n",
    "for col in columns_to_add:\n",
    "    ctdf_withcarb[col] = 0\n",
    "\n",
    "# Format FIPS codes\n",
    "ctdf_withcarb['fips'] = ctdf_withcarb.apply(lambda row: '0'+str(row['fips']) if len(str(row['fips']))==4 else str(row['fips']), axis=1)\n",
    "\n",
    "# Process data for each year\n",
    "for year in ['24','30','40']:\n",
    "    # Read all scenario data files\n",
    "    data_files = {\n",
    "        'UC': resdir+r'\\\\CostEmissionAssessment\\\\res_trb\\\\20'+year+'_0_0_0_0_3.csv',\n",
    "        'CC': resdir+r'\\\\CostEmissionAssessment\\\\res_trb\\\\20'+year+'_1_0_0_0_1.csv',\n",
    "        'V2H': resdir+r'\\\\CostEmissionAssessment\\\\res_trb\\\\20'+year+'_2_0_0_0_1.csv',\n",
    "        'V2Hnoelec': resdir+'\\\\CostEmissionAssessment\\\\res_trb\\\\20'+year+'_2_0_0_0_noelec_1.csv',\n",
    "        'V2H_cons': resdir+'\\\\CostEmissionAssessment\\\\res_trb\\\\20'+year+'_2_0_0_0_cons.csv'\n",
    "    }\n",
    "    \n",
    "    # Process each scenario\n",
    "    for scen, file_std in data_files.items():\n",
    "        df_std = pd.read_csv(file_std)\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if df_std['Cost'].isna().any() or df_std['Emissions'].isna().any():\n",
    "            print(f\"Warning: NaN values found in {scen} data\")\n",
    "            \n",
    "        # Assign values\n",
    "        # Map values based on county_num matching tempreg\n",
    "        for idx, row in df_std.iterrows():\n",
    "            mask = ctdf_withcarb['tempreg'] == row['county_num']\n",
    "            ctdf_withcarb.loc[mask, f'{year}{scen}costs'] = row['Cost']\n",
    "            ctdf_withcarb.loc[mask, f'{year}{scen}emissions'] = row['Emissions']\n",
    "\n",
    "# Calculate lifetime costs and emissions\n",
    "for scen in ['UC','CC','V2H','V2Hnoelec','V2H_cons']:\n",
    "    ctdf_withcarb[f'lca{scen}costs'] = 5 * (ctdf_withcarb[f'24{scen}costs'] + \n",
    "                                   ctdf_withcarb[f'30{scen}costs'] + \n",
    "                                   ctdf_withcarb[f'40{scen}costs'])\n",
    "    ctdf_withcarb[f'lca{scen}emissions'] = 5 * (ctdf_withcarb[f'24{scen}emissions'] + \n",
    "                                       ctdf_withcarb[f'30{scen}emissions'] + \n",
    "                                       ctdf_withcarb[f'40{scen}emissions'])\n",
    "\n",
    "# Calculate differences between scenarios (only for EV scenarios)\n",
    "ev_scenarios = ['UC','CC','V2Hnoelec','V2H','V2H_cons']\n",
    "for item in ['emissions','costs']:\n",
    "    for scen0 in ev_scenarios:\n",
    "        for scen1 in ev_scenarios:\n",
    "            if scen0 != scen1:\n",
    "                ctdf_withcarb[f'lca{scen0}{scen1}{item}'] = (\n",
    "                    ctdf_withcarb[f'lca{scen0}{item}'] - \n",
    "                    ctdf_withcarb[f'lca{scen1}{item}']\n",
    "                )\n",
    "\n",
    "# Add duplicate row and update index\n",
    "ctdf_withcarb = pd.concat([ctdf_withcarb, ctdf_withcarb.iloc[2330:2331]])\n",
    "ctdf_withcarb['index'] = np.arange(ctdf_withcarb.shape[0])\n",
    "ctdf_withcarb.set_index('index', inplace=True)\n",
    "ctdf_withcarb.loc[ctdf_withcarb.index[-1], 'fips'] = '46113'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5th: -2039.9840785347196\n",
      "Meanh: -1752.070659771265\n",
      "95th: -1356.1036406286703\n",
      "5th: -5989.2033782956505\n",
      "Meanh: -3897.988403343967\n",
      "95th: -2541.5789796467498\n",
      "5th: -5963.252204998398\n",
      "Meanh: -4326.065596685214\n",
      "95th: -3122.7828095237037\n"
     ]
    }
   ],
   "source": [
    "for scen in ['CCUC','V2HnoelecUC','V2HUC']:\n",
    "    values = ctdf_withcarb[f'lca{scen}costs'].dropna().sort_values().values\n",
    "    n = len(values)\n",
    "    # Find the closest observation to the 5th and 95th percentiles\n",
    "    p5_idx = int(round(0.05 * (n - 1)))\n",
    "    p95_idx = int(round(0.95 * (n - 1)))\n",
    "    mean_val = np.mean(values)\n",
    "    print(\"5th:\", values[p5_idx])\n",
    "    print(\"Meanh:\", mean_val)\n",
    "    print(\"95th:\", values[p95_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5th: -25.117012774003875\n",
      "Meanh: -21.808331384464946\n",
      "95th: -18.65070509128916\n",
      "5th: -83.42781937110504\n",
      "Meanh: -62.00900982965741\n",
      "95th: -39.077033696692396\n",
      "5th: -109.46296725977618\n",
      "Meanh: -86.33001523254039\n",
      "95th: -59.98520719619557\n"
     ]
    }
   ],
   "source": [
    "for scen in ['CCUC','V2HnoelecUC','V2HUC']:\n",
    "    values = ctdf_withcarb[f'lca{scen}emissions'].dropna().sort_values().values\n",
    "    n = len(values)\n",
    "    # Find the closest observation to the 5th and 95th percentiles\n",
    "    p5_idx = int(round(0.05 * (n - 1)))\n",
    "    p95_idx = int(round(0.95 * (n - 1)))\n",
    "    mean_val = np.mean(values)\n",
    "    print(\"5th:\", values[p5_idx]/1000)\n",
    "    print(\"Meanh:\", mean_val/1000)\n",
    "    print(\"95th:\", values[p95_idx]/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qual lifetime\n",
    "# EV aggregate\n",
    "resdir = r'F:\\University of Michigan Dropbox\\Jiahui Chen\\Ford_CC\\\\'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "# Read base data\n",
    "ctdf_nocarb = pd.read_csv(resdir+r'\\\\2020_Gaz_counties_national_0728_tempreg.csv')\n",
    "uqlst = np.unique(ctdf_nocarb['tempreg'])\n",
    "\n",
    "# Pre-allocate columns to avoid fragmentation\n",
    "columns_to_add = []\n",
    "for year in ['24','30','40']:\n",
    "    for scen in ['UC','CC','V2H','V2Hnoelec','V2H_cons']:\n",
    "        columns_to_add.extend([\n",
    "            f'{year}{scen}costs', f'{year}{scen}emissions'\n",
    "        ])\n",
    "\n",
    "# Initialize all columns with zeros\n",
    "for col in columns_to_add:\n",
    "    ctdf_nocarb[col] = 0\n",
    "\n",
    "# Format FIPS codes\n",
    "ctdf_nocarb['fips'] = ctdf_nocarb.apply(lambda row: '0'+str(row['fips']) if len(str(row['fips']))==4 else str(row['fips']), axis=1)\n",
    "\n",
    "# Process data for each year\n",
    "for year in ['24','30','40']:\n",
    "    # Read all scenario data files\n",
    "    data_files = {\n",
    "        'UC': resdir+r'\\\\CostEmissionAssessment\\\\res_trb\\\\20'+year+'_0_0_0_0_3.csv',\n",
    "        'CC': resdir+r'\\\\CostEmissionAssessment\\\\res_trb\\\\20'+year+'_1_0_0_0_nocarb.csv',\n",
    "        'V2H': resdir+r'\\\\CostEmissionAssessment\\\\res_trb\\\\20'+year+'_2_0_0_0_nocarb.csv',\n",
    "        'V2Hnoelec': resdir+'\\\\CostEmissionAssessment\\\\res_trb\\\\20'+year+'_2_0_0_0_noelec_nocarb.csv',\n",
    "        'V2H_cons': resdir+'\\\\CostEmissionAssessment\\\\res_trb\\\\20'+year+'_2_0_0_0_cons.csv'\n",
    "    }\n",
    "    \n",
    "    # Process each scenario\n",
    "    for scen, file_std in data_files.items():\n",
    "        df_std = pd.read_csv(file_std)\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if df_std['Cost'].isna().any() or df_std['Emissions'].isna().any():\n",
    "            print(f\"Warning: NaN values found in {scen} data\")\n",
    "            \n",
    "        # Assign values\n",
    "        # Map values based on county_num matching tempreg\n",
    "        for idx, row in df_std.iterrows():\n",
    "            mask = ctdf_nocarb['tempreg'] == row['county_num']\n",
    "            ctdf_nocarb.loc[mask, f'{year}{scen}costs'] = row['Cost']\n",
    "            ctdf_nocarb.loc[mask, f'{year}{scen}emissions'] = row['Emissions']\n",
    "\n",
    "# Calculate lifetime costs and emissions\n",
    "for scen in ['UC','CC','V2H','V2Hnoelec','V2H_cons']:\n",
    "    ctdf_nocarb[f'lca{scen}costs'] = 5 * (ctdf_nocarb[f'24{scen}costs'] + \n",
    "                                   ctdf_nocarb[f'30{scen}costs'] + \n",
    "                                   ctdf_nocarb[f'40{scen}costs'])\n",
    "    ctdf_nocarb[f'lca{scen}emissions'] = 5 * (ctdf_nocarb[f'24{scen}emissions'] + \n",
    "                                       ctdf_nocarb[f'30{scen}emissions'] + \n",
    "                                       ctdf_nocarb[f'40{scen}emissions'])\n",
    "\n",
    "# Calculate differences between scenarios (only for EV scenarios)\n",
    "ev_scenarios = ['UC','CC','V2Hnoelec','V2H','V2H_cons']\n",
    "for item in ['emissions','costs']:\n",
    "    for scen0 in ev_scenarios:\n",
    "        for scen1 in ev_scenarios:\n",
    "            if scen0 != scen1:\n",
    "                ctdf_nocarb[f'lca{scen0}{scen1}{item}'] = (\n",
    "                    ctdf_nocarb[f'lca{scen0}{item}'] - \n",
    "                    ctdf_nocarb[f'lca{scen1}{item}']\n",
    "                )\n",
    "\n",
    "# Add duplicate row and update index\n",
    "ctdf_nocarb = pd.concat([ctdf_nocarb, ctdf_nocarb.iloc[2330:2331]])\n",
    "ctdf_nocarb['index'] = np.arange(ctdf_nocarb.shape[0])\n",
    "ctdf_nocarb.set_index('index', inplace=True)\n",
    "ctdf_nocarb.loc[ctdf_nocarb.index[-1], 'fips'] = '46113'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5th: -2.11\n",
      "Meanh: -1.68\n",
      "95th: -1.17\n",
      "5th: -6.23\n",
      "Meanh: -4.24\n",
      "95th: -2.75\n",
      "5th: -6.11\n",
      "Meanh: -4.69\n",
      "95th: -3.45\n"
     ]
    }
   ],
   "source": [
    "for scen in ['CCUC','V2HnoelecUC','V2HUC']:\n",
    "    values = ctdf_nocarb[f'lca{scen}costs'].dropna().sort_values().values\n",
    "    n = len(values)\n",
    "    # Find the closest observation to the 5th and 95th percentiles\n",
    "    p5_idx = int(round(0.05 * (n - 1)))\n",
    "    p95_idx = int(round(0.95 * (n - 1)))\n",
    "    mean_val = np.mean(values)\n",
    "    print(\"5th:\", round(values[p5_idx]/1000, 2))\n",
    "    print(\"Meanh:\", round(mean_val/1000, 2))\n",
    "    print(\"95th:\", round(values[p95_idx]/1000, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in range(20)][10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C:\\\\Users\\\\jiahuic\\\\University of Michigan Dropbox\\\\Jiahui Chen\\\\Ford_CC\\\\Result_v2h\\\\Results_80_Y82_2024_nocarb',\n",
       "       'C:\\\\Users\\\\jiahuic\\\\University of Michigan Dropbox\\\\Jiahui Chen\\\\Ford_CC\\\\Result_v2h\\\\Results_80_Y82_2030_nocarb',\n",
       "       'C:\\\\Users\\\\jiahuic\\\\University of Michigan Dropbox\\\\Jiahui Chen\\\\Ford_CC\\\\Result_v2h\\\\Results_80_Y82_2040_nocarb',\n",
       "       'C:\\\\Users\\\\jiahuic\\\\University of Michigan Dropbox\\\\Jiahui Chen\\\\Ford_CC\\\\Result_v2h_no\\\\Results_2024_nocarb',\n",
       "       'C:\\\\Users\\\\jiahuic\\\\University of Michigan Dropbox\\\\Jiahui Chen\\\\Ford_CC\\\\Result_v2h_no\\\\Results_2030_nocarb',\n",
       "       'C:\\\\Users\\\\jiahuic\\\\University of Michigan Dropbox\\\\Jiahui Chen\\\\Ford_CC\\\\Result_v2h_no\\\\Results_2040_nocarb'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "oldfiles = pd.read_csv(r'F:\\University of Michigan Dropbox\\Jiahui Chen\\Ford_CC\\old_files_report.csv')\n",
    "oldfiles['directory'].apply(lambda x: os.path.dirname(x)).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     43\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file)\n\u001b[1;32m---> 44\u001b[0m     result \u001b[38;5;241m=\u001b[39m check_slowcr_vsoc(file_path)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m     46\u001b[0m         mismatches\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m, in \u001b[0;36mcheck_slowcr_vsoc\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_slowcr_vsoc\u001b[39m(file_path):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m         df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslowCR\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvSOC\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jiahuic\\AppData1\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\jiahuic\\AppData1\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\jiahuic\\AppData1\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\jiahuic\\AppData1\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the directory path\n",
    "base_dir = r'C:\\Users\\jiahuic\\University of Michigan Dropbox\\Jiahui Chen\\Ford_CC\\Result_v2h'\n",
    "\n",
    "# Function to check if slowCR equals vSOC difference when slowCR isn't 0\n",
    "def check_slowcr_vsoc(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'slowCR' not in df.columns or 'vSOC' not in df.columns:\n",
    "            return None\n",
    "        \n",
    "        # Calculate vSOC differences\n",
    "        vsoc_diff = df['vSOC'].diff().fillna(0)\n",
    "        \n",
    "        # Only compare where slowCR is not 0\n",
    "        mask = df['slowCR'] != 0\n",
    "        if not any(mask):\n",
    "            return None\n",
    "            \n",
    "        # Compare slowCR with vSOC differences only where slowCR isn't 0\n",
    "        is_equal = np.isclose(df.loc[mask, 'slowCR'], vsoc_diff[mask], rtol=1e-5, atol=1e-5)\n",
    "        \n",
    "        if not all(is_equal):\n",
    "            return {\n",
    "                'file': file_path,\n",
    "                'mismatch_indices': np.where(~is_equal)[0],\n",
    "                'slowCR_values': df.loc[mask, 'slowCR'][~is_equal].values,\n",
    "                'vsoc_diff_values': vsoc_diff[mask][~is_equal].values,\n",
    "                'hour_indices': df.index[mask][~is_equal].values\n",
    "            }\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return {'file': file_path, 'error': str(e)}\n",
    "\n",
    "# Walk through all directories and check CSV files\n",
    "mismatches = []\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            result = check_slowcr_vsoc(file_path)\n",
    "            if result:\n",
    "                mismatches.append(result)\n",
    "\n",
    "# Print results\n",
    "if mismatches:\n",
    "    print(\"Found mismatches between slowCR and vSOC differences (only where slowCR ≠ 0):\")\n",
    "    for mismatch in mismatches:\n",
    "        if 'error' in mismatch:\n",
    "            print(f\"\\nError in file {mismatch['file']}:\")\n",
    "            print(f\"Error message: {mismatch['error']}\")\n",
    "        else:\n",
    "            print(f\"\\nMismatches in file {mismatch['file']}:\")\n",
    "            print(\"Hour indices with mismatches:\", mismatch['hour_indices'])\n",
    "            print(\"slowCR values:\", mismatch['slowCR_values'])\n",
    "            print(\"vSOC diff values:\", mismatch['vsoc_diff_values'])\n",
    "            print(\"Absolute differences:\", np.abs(mismatch['slowCR_values'] - mismatch['vsoc_diff_values']))\n",
    "else:\n",
    "    print(\"No mismatches found between slowCR and vSOC differences in any files (where slowCR ≠ 0).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
